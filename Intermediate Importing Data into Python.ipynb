{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Intermediate Importing Data into Python (non local sources)\n",
    "    ##Importing Data from the Internet\n",
    "        #flat files\n",
    "        #HTTP GET requests\n",
    "        #Web Scraping\n",
    "    ##APIs to import data\n",
    "        #APIs and JSONs\n",
    "        #APIs and WWW\n",
    "    ##Dive into Twitter API    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing Data from the Internet - flat files and an excel file\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())\n",
    "\n",
    "#again\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame\n",
    "df = pd.read_csv(url, sep = ';')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column\n",
    "pd.DataFrame.hist(df.ix[:, 0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "#Excel\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file\n",
    "xls = pd.read_excel(url, sheet_name=None)\n",
    "\n",
    "# Print the sheetnames\n",
    "print(xls.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(xls['1700'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing Data from the Internet - HTTP GET requests\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# Package the request\n",
    "request = Request(\"https://campus.datacamp.com/courses/1606/4135?ex=2\")\n",
    "\n",
    "# send request and receive response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Close the response\n",
    "response.close()\n",
    "\n",
    "\n",
    "#again\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "\n",
    "request = Request(url)\n",
    "\n",
    "\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response\n",
    "html = response.read()\n",
    "\n",
    "\n",
    "print(html)\n",
    "\n",
    "#Close\n",
    "response.close()\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "r = requests.get(url)\n",
    "\n",
    "text = r.text\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing Data from the Internet - Web Scraping with BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "\n",
    "print(pretty_soup)\n",
    "\n",
    "\n",
    "# again\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "html_doc = r.text\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage\n",
    "guido_title = soup.title\n",
    "\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "print(guido_text)\n",
    "\n",
    "# again\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "html_doc = r.text\n",
    "\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags \n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs \n",
    "for link in a_tags:\n",
    "    print(link.get('href'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APIs to import data - APIs and JSONs\n",
    "\n",
    "# Load JSON\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair \n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n",
    "    \n",
    "\n",
    "import json\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "print(json_data['Title'])\n",
    "\n",
    "\n",
    "print(json_data['Year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to API in Python\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "print(r.text)\n",
    "\n",
    "\n",
    "#Again with decoding the JSON\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n",
    "\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary\n",
    "json_data = r.json()\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n",
    "\n",
    "    \n",
    "#Wikipedia API\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dive into Twitter API\n",
    "    \n",
    "import tweepy\n",
    "\n",
    "# Store OAuth authentication credentials in relevant variables - IDs hidden\n",
    "access_token = \"XXX\"\n",
    "access_token_secret = \"YYY\"\n",
    "consumer_key = \"AAA\"\n",
    "consumer_secret = \"BBB\"\n",
    "\n",
    "# OAuth handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Initialize Stream listener\n",
    "l = MyStreamListener()\n",
    "\n",
    "# Create your Stream object with authentication\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(track=['clinton', 'trump', 'sanders', 'cruz'])\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "tweets_data_path = 'tweets.txt'\n",
    "\n",
    "tweets_data = []\n",
    "\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang'])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])\n",
    "\n",
    "    \n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a list of labels:cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = sns.barplot(cd, [clinton, trump, sanders, cruz])\n",
    "ax.set(ylabel=\"count\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
